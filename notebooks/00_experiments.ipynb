{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCEPTS\n",
    "\n",
    "# hash all files in source and target\n",
    "# source = {hash: [set of paths to files with that hash]}\n",
    "# target = {hash: [set of paths to files with that hash]}\n",
    "# for hash in target:\n",
    "    # if hash not in source.keys():\n",
    "        # delete all files in target[hash]\n",
    "# for hash in source:\n",
    "    # if hash not in target.keys():\n",
    "        # copy all files in source[hash] to target\n",
    "    # elif source[hash] != target[hash]:\n",
    "        # new_files = source[hash] - target[hash]\n",
    "        # outdated_files = target[hash] - source[hash]\n",
    "        # for new_file in new_files:\n",
    "            # if len(outdated_files):\n",
    "                # file = outdated_files.pop()\n",
    "                # os.rename(file, new_file)\n",
    "            # else:\n",
    "                # copy new_file to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Set, Callable, Dict, List\n",
    "import hashlib\n",
    "from fnmatch import fnmatch\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import abc\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileHasher(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, file_path:Path) -> str:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModificationStampHasher(FileHasher):\n",
    "    def __init__(self, base_path: Path) -> None:\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __call__(self, file_path:Path) -> str:\n",
    "        rel_path = file_path.relative_to(self.base_path)\n",
    "        return f\"{rel_path}_{os.path.getmtime(file_path)}\"\n",
    "\n",
    "class MD5Hasher(FileHasher):\n",
    "    def __call__(self, file_path:Path) -> str:\n",
    "        \"\"\"Return the md5 hash of a file.\"\"\"\n",
    "        hasher = hashlib.md5()\n",
    "        block_size = 64 * 1024\n",
    "\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            while True:\n",
    "                data = file.read(block_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                hasher.update(data)\n",
    "\n",
    "        return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_file(file_path: Path, excluded_paths: List[Path] = None) -> bool:\n",
    "    \"\"\"Return True if file is valid, False otherwise.\"\"\"\n",
    "    excluded_paths = excluded_paths or []\n",
    "    try:\n",
    "        if file_path.is_file() and not any(\n",
    "            fnmatch(str(file_path), str(excluded_path)) for excluded_path in excluded_paths\n",
    "        ):\n",
    "            return True\n",
    "    except OSError:\n",
    "        logging.warning(f\"Could not read {file_path}\")\n",
    "\n",
    "\n",
    "def get_all_files(folder_path: Path, excluded_paths: List[Path] = None, n_threads: int = 1) -> List[Path]:\n",
    "    \"\"\"Return a list of all valid files in a folder.\"\"\"\n",
    "    paths = list(folder_path.rglob(\"*\"))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        with tqdm(total=len(paths)) as pbar:\n",
    "            futures = []\n",
    "            for p in paths:\n",
    "                future = executor.submit(validate_file, p, excluded_paths)\n",
    "                future.add_done_callback(lambda _: pbar.update())\n",
    "                futures.append(future)\n",
    "\n",
    "            paths = [p for p, future in zip(paths, futures) if future.result()]\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes(\n",
    "    file_paths: List[Path], file_hash_func = None, n_threads: int = 1\n",
    ") -> Dict[Path, str]:\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        with tqdm(total=len(file_paths)) as pbar:\n",
    "            \n",
    "            futures = []\n",
    "            for file_path in file_paths:\n",
    "                future = executor.submit(file_hash_func, file_path)\n",
    "                future.add_done_callback(lambda _: pbar.update())\n",
    "                futures.append(future)\n",
    "\n",
    "            file_hashes = [future.result() for future in futures]\n",
    "\n",
    "    result = dict(zip(file_paths, file_hashes))\n",
    "\n",
    "    return result\n",
    "\n",
    "# def get_file_hashes(\n",
    "#     folder_path: Path, file_hash_func: Callable = md5_file_hash, n_threads: int = 1\n",
    "# ) -> Dict[Path, str]:\n",
    "#     \"\"\"Return a dictionary of file hashes for all files in a folder.\"\"\"\n",
    "#     file_paths = get_all_files(folder_path, n_threads=n_threads)\n",
    "#     return _get_file_hashes(file_paths, file_hash_func, n_threads=n_threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2291194104.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 59\u001b[1;36m\u001b[0m\n\u001b[1;33m    #     target_hashes[hash] = source_hashes[hash]\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Syncer:\n",
    "    def __init__(self, source_folder, target_folder) -> None:\n",
    "        self.source_folder = source_folder\n",
    "        self.target_folder = target_folder\n",
    "\n",
    "    def __call__(self) -> None:\n",
    "        \n",
    "\n",
    "def get_source_and_target_hashes(\n",
    "    source_folder_path: Path,\n",
    "    target_folder_path: Path,\n",
    "    file_hash_func: Callable = md5_file_hash,\n",
    "    n_threads: int = 1,\n",
    "    excluded_paths: List[Path] = None,\n",
    "):\n",
    "    logging.info(f\"Syncing {source_folder_path} to {target_folder_path}\")\n",
    "\n",
    "    logging.info(\"Getting all valid files in source folder...\")\n",
    "    source_file_paths = get_all_files(\n",
    "        source_folder_path, excluded_paths=excluded_paths, n_threads=n_threads\n",
    "    )\n",
    "\n",
    "    logging.info(\"Hashing all valid files in source folder...\")\n",
    "    source_hashes = get_file_hashes(\n",
    "        source_file_paths, file_hash_func, n_threads=n_threads\n",
    "    )\n",
    "\n",
    "    logging.info(\"Getting all valid files in target folder...\")\n",
    "    target_file_paths = get_all_files(\n",
    "        target_folder_path, excluded_paths=excluded_paths, n_threads=n_threads\n",
    "    )\n",
    "\n",
    "    logging.info(\"Hashing all valid files in target folder...\")\n",
    "    target_hashes = get_file_hashes(\n",
    "        target_file_paths, file_hash_func, n_threads=n_threads\n",
    "    )\n",
    "\n",
    "    return source_hashes, target_hashes\n",
    "\n",
    "\n",
    "def sync_by_path(source_hashes, target_hashes):\n",
    "    # first delete all files in target that are not in source\n",
    "    for path, hash in target_hashes.items():\n",
    "        if path not in source_hashes.keys():\n",
    "            (target_folder / path).unlink()\n",
    "            \n",
    "\n",
    "    for path, hash in source_hashes.items():\n",
    "        if path in target_hashes.keys():\n",
    "            if hash != target_hashes[path]:\n",
    "                \n",
    "        if path not in target_hashes.keys():\n",
    "            \n",
    "    \n",
    "    # for path, hash in source_hashes.items():\n",
    "    #     if path not in target_hashes.keys():\n",
    "    #         yield path, hash\n",
    "    # set(source_hashes.items())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # outdated_files = target_hashes.keys() - source_hashes.keys()\n",
    "    # logging.info(f'Deleting {len(outdated_files)} files from target folder...')\n",
    "    # for hash in tqdm(outdated_files):\n",
    "    #     for file_path in target_hashes[hash]:\n",
    "    #         file_path.unlink()\n",
    "    #     target_hashes.pop(hash)\n",
    "\n",
    "    # new_files = source_hashes.keys() - target_hashes.keys()\n",
    "    # logging.info(f'Copying {len(new_files)} files to target folder...')\n",
    "    # for hash in tqdm(new_files):\n",
    "    #     for file_path in source_hashes[hash]:\n",
    "    #         # file_path.rename(target_folder_path / file_path.name)\n",
    "    #     target_hashes[hash] = source_hashes[hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"D:/Dokumente/\") # 52GB, 353.000 Dateien\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Syncing D:\\Dokumente to None\n",
      "INFO:root:Getting all valid files in source folder...\n",
      " 51%|█████     | 201859/394935 [01:19<00:35, 5437.70it/s]WARNING:root:Could not read D:\\Dokumente\\Uni\\HiWi\\scenario-visualization\\venv_wsl\\bin\\python3\n",
      "WARNING:root:Could not read D:\\Dokumente\\Uni\\HiWi\\scenario-visualization\\venv_wsl\\bin\\python\n",
      "WARNING:root:Could not read D:\\Dokumente\\Uni\\HiWi\\scenario-visualization\\venv_wsl\\bin\\python3.8\n",
      "100%|██████████| 394935/394935 [02:00<00:00, 3280.28it/s]\n",
      "INFO:root:Hashing all valid files in source folder...\n",
      "  0%|          | 278/353731 [00:16<5:41:32, 17.25it/s] \n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "sync_folders(p, None, n_threads=4, excluded_paths=None, file_hash_func=modification_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e41f63829d8e97276cbb1f7b474b7c30d3a33e7416d9a01b5ea5b6b03cd367df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
