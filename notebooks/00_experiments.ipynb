{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Set\n",
    "import hashlib\n",
    "from fnmatch import fnmatch\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import abc\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileHasher(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, file_path:Path) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ModificationStampHasher(FileHasher):\n",
    "    def __init__(self, base_path: Path) -> None:\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __call__(self, file_path:Path) -> str:\n",
    "        rel_path = file_path.relative_to(self.base_path)\n",
    "        return f\"{rel_path}_{os.path.getmtime(file_path)}\"\n",
    "\n",
    "class MD5Hasher(FileHasher):\n",
    "    def __call__(self, file_path:Path) -> str:\n",
    "        \"\"\"Return the md5 hash of a file.\"\"\"\n",
    "        hasher = hashlib.md5()\n",
    "        block_size = 64 * 1024\n",
    "\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            while True:\n",
    "                data = file.read(block_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                hasher.update(data)\n",
    "\n",
    "        return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Syncer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_folder: Path,\n",
    "        target_folder: Path,\n",
    "        excluded_paths: List[Path] = None,\n",
    "        hashing_method: str = \"md5\",\n",
    "        sync_method: str = \"path\",\n",
    "        n_threads: int = 1,\n",
    "    ) -> None:\n",
    "        self.source_folder = source_folder\n",
    "        self.target_folder = target_folder\n",
    "        self.excluded_paths = excluded_paths or []\n",
    "        self.n_threads = n_threads\n",
    "\n",
    "        self.sync_method = None\n",
    "        if sync_method == \"path\":\n",
    "            self.sync_method = self._sync_by_path\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sync method {sync_method}\")\n",
    "\n",
    "        self.source_file_hash_func: FileHasher = None\n",
    "        self.target_file_hash_func: FileHasher = None\n",
    "        if hashing_method == \"md5\":\n",
    "            self.source_file_hash_func = MD5Hasher()\n",
    "            self.target_file_hash_func = MD5Hasher()\n",
    "        elif hashing_method == \"metadata\":\n",
    "            self.source_file_hash_func = ModificationStampHasher(self.source_folder)\n",
    "            self.target_file_hash_func = ModificationStampHasher(self.target_folder)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown hashing method {hashing_method}\")\n",
    "\n",
    "    def _validate_file(self, file_path: Path) -> bool:\n",
    "        \"\"\"Return True if file is valid, False otherwise.\"\"\"\n",
    "        try:\n",
    "            if file_path.is_file() and not any(\n",
    "                fnmatch(str(file_path), str(excluded_path))\n",
    "                for excluded_path in self.excluded_paths\n",
    "            ):\n",
    "                return True\n",
    "        except OSError:\n",
    "            logging.warning(f\"Could not read {file_path}\")\n",
    "\n",
    "    def _all_files(self, folder_path: Path) -> List[Path]:\n",
    "        \"\"\"Return a list of all valid files in a folder.\"\"\"\n",
    "        paths = list(folder_path.rglob(\"*\"))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.n_threads) as executor:\n",
    "            with tqdm(total=len(paths)) as pbar:\n",
    "                futures = []\n",
    "                for p in paths:\n",
    "                    future = executor.submit(self._validate_file, p)\n",
    "                    future.add_done_callback(lambda _: pbar.update())\n",
    "                    futures.append(future)\n",
    "\n",
    "                paths = [p for p, future in zip(paths, futures) if future.result()]\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def _path_hash_mapping(\n",
    "        self, file_paths: List[Path], file_hash_func\n",
    "    ) -> Dict[Path, str]:\n",
    "        with ThreadPoolExecutor(max_workers=self.n_threads) as executor:\n",
    "            with tqdm(total=len(file_paths)) as pbar:\n",
    "                futures = []\n",
    "                for file_path in file_paths:\n",
    "                    future = executor.submit(file_hash_func, file_path)\n",
    "                    future.add_done_callback(lambda _: pbar.update())\n",
    "                    futures.append(future)\n",
    "\n",
    "                file_hashes = [future.result() for future in futures]\n",
    "\n",
    "        result = dict(zip(file_paths, file_hashes))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _relative_path_hash_mapping(self, mapping, base_path) -> Dict[Path, str]:\n",
    "        result = {}\n",
    "        for path, hash in mapping.items():\n",
    "            rel_path = path.relative_to(base_path)\n",
    "            result[rel_path] = hash\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _sync_by_path(\n",
    "        self,\n",
    "        source_path_hash_mapping: Dict[Path, str],\n",
    "        target_path_hash_mapping: Dict[Path, str],\n",
    "    ) -> None:\n",
    "        statistics = {\"unchanged\": 0, \"changed\": 0, \"new\": 0, \"removed\": 0}\n",
    "\n",
    "        logging.info(\"Deleting files in target that are not in source...\")\n",
    "        for rel_path, hash in tqdm(target_path_hash_mapping.items()):\n",
    "            if rel_path not in source_path_hash_mapping.keys():\n",
    "                (self.target_folder / rel_path).unlink()\n",
    "                statistics[\"removed\"] += 1\n",
    "\n",
    "        logging.info(\"Copying files from source to target...\")\n",
    "        for rel_path, hash in tqdm(source_path_hash_mapping.items()):\n",
    "            if rel_path in target_path_hash_mapping.keys():\n",
    "                if hash != target_path_hash_mapping[rel_path]:\n",
    "                    (self.target_folder / rel_path).unlink()\n",
    "                    shutil.copy2(\n",
    "                        self.source_folder / rel_path,\n",
    "                        (self.target_folder / rel_path).parent,\n",
    "                    )\n",
    "                    statistics[\"changed\"] += 1\n",
    "                else:\n",
    "                    statistics[\"unchanged\"] += 1\n",
    "\n",
    "            else:\n",
    "                (self.target_folder / rel_path).parent.mkdir(\n",
    "                    parents=True, exist_ok=True\n",
    "                )\n",
    "                shutil.copy2(\n",
    "                    self.source_folder / rel_path,\n",
    "                    (self.target_folder / rel_path).parent,\n",
    "                )\n",
    "                statistics[\"new\"] += 1\n",
    "\n",
    "        logging.info(f\"Synced {sum(statistics.values())} files:\")\n",
    "        for key, value in statistics.items():\n",
    "            logging.info(f\"{key}: {value}\")\n",
    "\n",
    "    def _invert_mapping(self, mapping: Dict[Path, str]) -> Dict[str, List[Path]]:\n",
    "        result = defaultdict(list)\n",
    "        for path, hash in mapping.items():\n",
    "            result[hash].append(path)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _sync_by_hash(\n",
    "        source_path_hash_mapping: Dict[Path, str],\n",
    "        target_path_hash_mapping: Dict[Path, str],\n",
    "    ) -> None:\n",
    "        # will be faster when large subtrees are moved or renamed but the file contents are the same\n",
    "        # not tested yet\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "        source_hash_paths_mapping = self._invert_mapping(source_path_hash_mapping)\n",
    "        target_hash_paths_mapping = self._invert_mapping(target_path_hash_mapping)\n",
    "\n",
    "        statistics = {\"unchanged\": 0, \"changed\": 0, \"new\": 0, \"removed\": 0}\n",
    "\n",
    "        logging.info(\"Deleting files in target that are not in source...\")\n",
    "        for hash, paths in tqdm(target_hash_paths_mapping.items()):\n",
    "            if hash not in source_hash_paths_mapping.keys():\n",
    "                for path in paths:\n",
    "                    path.unlink()\n",
    "                    statistics[\"removed\"] += 1\n",
    "\n",
    "        logging.info(\"Copying files from source to target...\")\n",
    "        for hash, paths in tqdm(source_hash_paths_mapping.items()):\n",
    "            if hash not in target_hash_paths_mapping.keys():\n",
    "                for path in paths:\n",
    "                    (self.target_folder / path).parent.mkdir(\n",
    "                        parents=True, exist_ok=True\n",
    "                    )\n",
    "                    shutil.copy2(\n",
    "                        self.source_folder / path, (self.target_folder / path).parent\n",
    "                    )\n",
    "                    statistics[\"new\"] += 1\n",
    "                continue\n",
    "            \n",
    "            if paths == target_hash_paths_mapping[hash]:\n",
    "                statistics[\"unchanged\"] += len(paths)\n",
    "                continue\n",
    "\n",
    "            new_paths = paths - target_hash_paths_mapping[hash]\n",
    "            old_paths = target_hash_paths_mapping[hash] - paths\n",
    "            for path in new_paths:\n",
    "                (self.target_folder / path).parent.mkdir(\n",
    "                    parents=True, exist_ok=True\n",
    "                )\n",
    "                if len(old_paths) > 0:\n",
    "                    shutil.move(\n",
    "                        self.source_folder / old_paths.pop(),\n",
    "                        self.target_folder / path,\n",
    "                    )\n",
    "                    statistics[\"moved\"] += 1\n",
    "                else:\n",
    "                    shutil.copy2(\n",
    "                        self.source_folder / path,\n",
    "                        (self.target_folder / path).parent,\n",
    "                    )\n",
    "                    statistics[\"new\"] += 1\n",
    "\n",
    "        logging.info(f\"Synced {sum(statistics.values())} files:\")\n",
    "        for key, value in statistics.items():\n",
    "            logging.info(f\"{key}: {value}\")\n",
    "\n",
    "    def __call__(self) -> None:\n",
    "        logging.info(f\"Syncing {self.source_folder} to {self.target_folder}\")\n",
    "\n",
    "        logging.info(\"Getting all valid files in source folder...\")\n",
    "        source_file_paths = self._all_files(self.source_folder)\n",
    "        logging.info(\"Hashing all valid files in source folder...\")\n",
    "        source_path_hash_mapping = self._path_hash_mapping(\n",
    "            source_file_paths, self.source_file_hash_func\n",
    "        )\n",
    "        logging.info(\"Making paths relative to source folder...\")\n",
    "        source_path_map_hashing = self._relative_path_hash_mapping(\n",
    "            source_path_hash_mapping, self.source_folder\n",
    "        )\n",
    "\n",
    "        logging.info(\"Getting all valid files in target folder...\")\n",
    "        target_file_paths = self._all_files(self.target_folder)\n",
    "        logging.info(\"Hashing all valid files in target folder...\")\n",
    "        target_path_hash_mapping = self._path_hash_mapping(\n",
    "            target_file_paths, self.target_file_hash_func\n",
    "        )\n",
    "        logging.info(\"Making paths relative to target folder...\")\n",
    "        target_path_hash_mapping = self._relative_path_hash_mapping(\n",
    "            target_path_hash_mapping, self.target_folder\n",
    "        )\n",
    "\n",
    "        logging.info(\"Syncing files...\")\n",
    "        self.sync_method(source_path_map_hashing, target_path_hash_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Syncing d:\\Dokumente\\Software_Projekte\\FolderSync\\tests\\test_data\\semester_10 to d:\\Dokumente\\Software_Projekte\\FolderSync\\tests\\test_data\\semester_10_copy\n",
      "INFO:root:Getting all valid files in source folder...\n",
      "100%|██████████| 2165/2165 [00:00<00:00, 3567.50it/s]\n",
      "INFO:root:Hashing all valid files in source folder...\n",
      "100%|██████████| 1583/1583 [00:00<00:00, 2155.54it/s]\n",
      "INFO:root:Making paths relative to source folder...\n",
      "INFO:root:Getting all valid files in target folder...\n",
      "100%|██████████| 2118/2118 [00:00<00:00, 4540.57it/s]\n",
      "INFO:root:Hashing all valid files in target folder...\n",
      "100%|██████████| 1582/1582 [00:00<00:00, 3638.45it/s]\n",
      "INFO:root:Making paths relative to target folder...\n",
      "INFO:root:Syncing files...\n",
      "INFO:root:Deleting files in target that are not in source...\n",
      "100%|██████████| 1582/1582 [00:00<00:00, 264316.00it/s]\n",
      "INFO:root:Copying files from source to target...\n",
      "100%|██████████| 1583/1583 [00:00<00:00, 83563.02it/s]\n",
      "INFO:root:Synced 1583 files:\n",
      "INFO:root:unchanged: 1582\n",
      "INFO:root:changed: 0\n",
      "INFO:root:new: 1\n",
      "INFO:root:removed: 0\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "source_path = Path.cwd().parent / \"tests\" / \"test_data\" / \"semester_10\"\n",
    "target_path = Path.cwd().parent / \"tests\" / \"test_data\" / \"semester_10_copy\"\n",
    "\n",
    "syncer = Syncer(\n",
    "    source_folder=source_path,\n",
    "    target_folder=target_path,\n",
    "    excluded_paths=None,\n",
    "    sync_method=\"path\",\n",
    "    hashing_method=\"metadata\",\n",
    "    n_threads=4,\n",
    ")\n",
    "\n",
    "syncer()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e41f63829d8e97276cbb1f7b474b7c30d3a33e7416d9a01b5ea5b6b03cd367df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
